{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)\n",
    "# split each line on \",\" and then for each line, create a dictionary\n",
    "# where the key is the header title of the field, and the value is the value of that field in the row.\n",
    "# The function parse_file should return a list of dictionaries,\n",
    "# each data line in the file being a single list entry.\n",
    "# Field names and values should not contain extra whitespace, like spaces or newline characters.\n",
    "# You can use the Python string method strip() to remove the extra whitespace.\n",
    "# You have to parse only the first 10 data lines in this exercise,\n",
    "# so the returned list should have 10 entries!\n",
    "import os\n",
    "\n",
    "DATADIR = \"/Users/josemanuelfernandez/Documents/Udacity/Data_Analyst/P3/Data-Wrangling/Lesson-1/\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    # 'rb' 'reads-binary': allows python to be more flexible in reading what's in the file.\n",
    "    with open(datafile, \"rb\") as f:\n",
    "        \n",
    "        # The method readlines() reads until EOF using readline() and returns a list \n",
    "        # containing the lines.\n",
    "        # Reads first line of file and split it using \",\"\n",
    "        # This gives you a list of values you can use as 'keys items' for the \n",
    "        # data items you pull on from the data file later on.\n",
    "        header = f.readline().split(\",\") # By getting the 'headers' you use them as 'key'\n",
    "                                         # and the lines split (by commas) as values\n",
    "        #print header # These are the keys\n",
    "        \n",
    "        counter = 0\n",
    "        for line in f: # Loop over the lines on the file 'f'\n",
    "            if counter == 10: # Counter-> Makes sure considers up to the 10th line (not inclusive)\n",
    "                break # Break if we have read 10 lines\n",
    "            \n",
    "            #print line # Execute-> you only see each line separated by commas\n",
    "            \n",
    "            # For every line up to the 10th line \n",
    "            # We split the line again using the comma delimeter\n",
    "            fields = line.split(',')\n",
    "            #print fields # 'fields' are lists with the values\n",
    "            entry = {} # Initialize an empty dictionary. The entry is going to be the data item\n",
    "                       # that will construct using the 'keys' we got from the first \n",
    "                       # line of the file ('header') and the indiviual line we processed obove (field)\n",
    "            \n",
    "            # Constructs the dictionary with the key-value pais.\n",
    "            # By using 'enumerate' we get an 'index' value in addition to a value \n",
    "            # for each item in the 'fields' list\n",
    "            for i, value in enumerate(fields):\n",
    "                #print i, value # i-> from 0-10 fields/variables (keys); \n",
    "                                # value-> each corresponding value per line\n",
    "                entry[header[i].strip()] = value.strip() # Assigns appropiate 'value' corresponding to each\n",
    "                                            # field ('header') for that the i_th key for that particular field\n",
    "            # Use 'strip()' to clean any empty space\n",
    "            \n",
    "            data.append(entry)\n",
    "            counter += 1\n",
    "                    \n",
    "    return data\n",
    "\n",
    "#parse_file(DATAFILE)\n",
    "\n",
    "## Test program\n",
    "\"\"\"\n",
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "test()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using CSV Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-ea2750eb6c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \"\"\"\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATAFILE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-ea2750eb6c21>\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(datafile)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 'rb' 'reads-binary': allows python to be more flexible in reading what's in the file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Assumes we want to read all our data into dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'b'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import csv # https://docs.python.org/2/library/csv.html\n",
    "\n",
    "DATADIR = \"/Users/josemanuelfernandez/Documents/Udacity/Data_Analyst/P3/Data-Wrangling/Lesson-1/\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    n = 0\n",
    "    # 'rb' 'reads-binary': allows python to be more flexible in reading what's in the file.\n",
    "    with open(datafile, \"rb\") as sd:\n",
    "        \n",
    "        r = csv.DictReader(sd) # Assumes we want to read all our data into dictionaries\n",
    "                                # Assumes first row contains headers and those names we\n",
    "                                # want to use as \"fields\"\n",
    "                                # Creates a dictionary for each row and keys will be the fields\n",
    "                                # from the headers and values would be the rows associated with them.\n",
    "        \n",
    "        for line in r: # Loop through the dictionaries 'r'\n",
    "            data.append(line)              \n",
    "    return data\n",
    "\n",
    "\"\"\"\n",
    "# Print out all of those values\n",
    "if __name__ == '__name__':\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    parse_csv(datafile)\n",
    "    d = parse_csv(datafile)\n",
    "    pprint.pprint(d)\n",
    "\"\"\"\n",
    "\n",
    "parse_file(DATAFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with XLS - Intro to XRLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List Comprehension\n",
      "data[3][2]: 1036.088697\n",
      "\n",
      "Cells in a nested loop:\n",
      "41277.0833333 9238.73731 1438.20528 1565.442856 916.708348 14010.903488 3027.98334 6165.211119 1157.741663 37520.933404 \n",
      "ROWS, COLUMNS, and CELLS:\n",
      "Number of Columns in the sheet: 10\n",
      "Number of rows in the sheet: 7296\n",
      "Type of data in cell (row 3, col 2): 2\n",
      "Value in cell (row 3, col 2): 1036.088697\n",
      "Get a slice of values in column 3, from rows 1-3:\n",
      "[1411.7505669999982, 1403.4722870000019, 1395.053150000001]\n",
      "\n",
      "DATES:\n",
      "Type of data in cell (row 1, col 0): 3\n",
      "Time in Excel format: 41275.0416667\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 1, 1, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Install the xlrd library locally: pip install xlrd\n",
    "import xlrd #module reads xlsx or xls\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    data = [[sheet.cell_value(r, col) \n",
    "                for col in range(sheet.ncols)] \n",
    "                    for r in range(sheet.nrows)]\n",
    "\n",
    "    print \"\\nList Comprehension\"\n",
    "    print \"data[3][2]:\",\n",
    "    print data[3][2]\n",
    "\n",
    "    print \"\\nCells in a nested loop:\"    \n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print sheet.cell_value(row, col),\n",
    "\n",
    "\n",
    "    ### other useful methods:\n",
    "    print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    print \"Number of Columns in the sheet:\",\n",
    "    print sheet.ncols\n",
    "    print \"Number of rows in the sheet:\", \n",
    "    print sheet.nrows\n",
    "    print \"Type of data in cell (row 3, col 2):\", \n",
    "    print sheet.cell_type(3, 2)\n",
    "    print \"Value in cell (row 3, col 2):\", \n",
    "    print sheet.cell_value(3, 2)\n",
    "    print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    print \"\\nDATES:\"\n",
    "    print \"Type of data in cell (row 1, col 0):\", \n",
    "    print sheet.cell_type(1, 0)\n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "    print \"Time in Excel format:\",\n",
    "    print exceltime\n",
    "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Excel Files - Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avgcoast': 10976.933460679751,\n",
       " 'maxtime': (2013, 8, 13, 17, 0, 0),\n",
       " 'maxvalue': 18779.025510000003,\n",
       " 'mintime': (2013, 2, 3, 4, 0, 0),\n",
       " 'minvalue': 6602.113898999982}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format\n",
    "\"\"\"\n",
    "\n",
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "        \n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0) # Reads first sheet\n",
    "\n",
    "    ### example on how you can get the data\n",
    "    sheet_data = [[sheet.cell_value(r, col) for col in \n",
    "                   range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "    \n",
    "    cv = sheet.col_values(1, start_rowx=1, end_rowx=None)\n",
    "    \n",
    "    maxval = max(cv)\n",
    "    minval = min(cv)\n",
    "    \n",
    "    maxpos = cv.index(maxval) + 1 # Obs. start at row=1 (index starts at row =0) \n",
    "                                  # so, position is +1\n",
    "    minpos = cv.index(minval) + 1 # Same for 'minpos'\n",
    "    #print (maxpos)\n",
    "    #print (minpos)\n",
    "    \n",
    "    maxtime = sheet.cell_value(maxpos, 0) # \"Value in row-cell = 'maxpos' \n",
    "                                          # and col 0 for column with time-data:\"\n",
    "    realmaxtime = xlrd.xldate_as_tuple(maxtime, 0) # Converts 'maxtime' into a tuple with time format\n",
    "    \n",
    "    mintime = sheet.cell_value(minpos, 0) # Same as before\n",
    "    realmintime = xlrd.xldate_as_tuple(mintime, 0) # Converts 'mintime' into a tuple with time format\n",
    "    \n",
    "    ### other useful methods:\n",
    "    # print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    # print \"Number of rows in the sheet:\", \n",
    "    # print sheet.nrows\n",
    "    # print \"Type of data in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_type(3, 2)\n",
    "    # print \"Value in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_value(3, 2)\n",
    "    # print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    # print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    # print \"\\nDATES:\"\n",
    "    # print \"Type of data in cell (row 1, col 0):\", \n",
    "    # print sheet.cell_type(1, 0)\n",
    "    # exceltime = sheet.cell_value(1, 0)\n",
    "    # print \"Time in Excel format:\",\n",
    "    # print exceltime\n",
    "    # print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    # print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "            'maxtime': realmaxtime,\n",
    "            'maxvalue': maxval,\n",
    "            'mintime': realmintime,\n",
    "            'minvalue': minval,\n",
    "            'avgcoast': sum(cv) / float(len(cv))\n",
    "    }\n",
    "    return data\n",
    "\n",
    "parse_file(datafile)\n",
    "\n",
    "\n",
    "## Test script\n",
    "#def test():\n",
    "#    #open_zip(datafile)\n",
    "#    data = parse_file(datafile)\n",
    "\n",
    "#    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "#    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Intro to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To experiment with this code freely you will have to run this code locally.\n",
    "# Take a look at the main() function for an example of how to use the code.\n",
    "# We have provided example json output in the other code editor tabs for you to\n",
    "# look at, but you will not be able to run any queries through our UI.\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "    print \"\\nARTIST:\"\n",
    "    pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "    print \"\\nONE RELEASE:\"\n",
    "    pretty_print(releases[0], indent=2)\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "    print \"\\nALL TITLES:\"\n",
    "    for t in release_titles:\n",
    "        print t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Using CSV Module -- Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "\n",
    "DATADIR = \"/Users/josemanuelfernandez/Documents/Udacity/Data_Analyst/P3/Data-Wrangling/Lesson-1/code-master/Lesson_1_Problem_Set/01-Using_CSV_Module/\"\n",
    "DATAFILE = \"745090.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    #with open(datafile,'r', newline='') as f: # Python 3\n",
    "    with open(datafile,'rb') as f:\n",
    "        header = f.readline().split(\",\")\n",
    "        name = header[1].replace('\"','')\n",
    "        #name = name.replace('\"','')\n",
    "        has_header = csv.Sniffer().has_header(f.read(1024))\n",
    "        #print (has_header)\n",
    "        #print (f.seek(0))  # rewind\n",
    "        reader = csv.reader(f) # Each row read from the csv file is returned as a list of strings.\n",
    "        if has_header:\n",
    "            next(reader)  # skip header row\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "        #pass\n",
    "    # Do not change the line below\n",
    "    #print (data)\n",
    "    return (name, data)\n",
    "\n",
    "#name, data = parse_file(\"745090.csv\")\n",
    "\n",
    "#print (data[0][1])\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "\n",
    "    #assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"01:00\"\n",
    "    #assert data[2][0] == \"01/01/2005\"\n",
    "    #assert data[2][5] == \"2\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Excel to CSV -- Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "    \n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = []\n",
    "    #data_dict = {}\n",
    "\n",
    "    for col in range(1, (sheet.ncols - 1)): # Only considers col 1-9 (stations)\n",
    "        cv = sheet.col_values(col, start_rowx=1, end_rowx=None)\n",
    "        maxload = max(cv)\n",
    "        maxpos = cv.index(maxload) + 1 # Obs. start at row=1 (index starts at row =0)\n",
    "                                  # so, position is +1\n",
    "        maxtime = sheet.cell_value(maxpos, 0) # \"Value in row-cell = 'maxpos'\n",
    "                                          # and col 0 for column with time-data:\"\n",
    "        realmaxtime = xlrd.xldate_as_tuple(maxtime, 0) # Converts 'maxtime' into a tuple with time format\n",
    "        station = sheet.cell_value(0,col)\n",
    "        data_dict = {\n",
    "            'Station': station,\n",
    "            'Year': realmaxtime[0],\n",
    "            'Month': realmaxtime[1],\n",
    "            'Day': realmaxtime[2],\n",
    "            'Hour': realmaxtime[3],\n",
    "            'Max Load': maxload}\n",
    "        data.append(data_dict)\n",
    "    # YOUR CODE HERE\n",
    "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
    "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
    "    return (data)\n",
    "\n",
    "def save_file(data, filename):\n",
    "    #filename = ''.join((filename, '.csv'))\n",
    "    #header = ['Station', 'Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    #with open(filename,'w', newline='') as csvfile: # Python 3.6\n",
    "    with open(filename,'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=data[0].keys(),\n",
    "                            delimiter='|')\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    \n",
    "def test():\n",
    "    open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Solution: Possible Solution for -- Excel to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##One of possible solutions is below:\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = {}\n",
    "    # process all rows that contain station data\n",
    "    for n in range (1, 9):\n",
    "        station = sheet.cell_value(0, n)\n",
    "        cv = sheet.col_values(n, start_rowx=1, end_rowx=None)\n",
    "\n",
    "        maxval = max(cv)\n",
    "        maxpos = cv.index(maxval) + 1\n",
    "        maxtime = sheet.cell_value(maxpos, 0)\n",
    "        realtime = xlrd.xldate_as_tuple(maxtime, 0)\n",
    "        data[station] = {\"maxval\": maxval,\n",
    "                         \"maxtime\": realtime}\n",
    "\n",
    "    print data\n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        w = csv.writer(f, delimiter='|')\n",
    "        w.writerow([\"Station\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Max Load\"])\n",
    "        for s in data:\n",
    "            year, month, day, hour, _ , _= data[s][\"maxtime\"]\n",
    "            w.writerow([s, year, month, day, hour, data[s][\"maxval\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Wrangling JSON -- Exercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular-viewed-1.json\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'popular-viewed-1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-c7f3d350d3cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"viewed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-c7f3d350d3cf>\u001b[0m in \u001b[0;36mget_from_file\u001b[0;34m(kind, period)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"popular-{0}-{1}.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'popular-viewed-1.json'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site\n",
    "and get your own API key. You will be able to complete this exercise in our UI\n",
    "without doing so, as we have provided a sample result. (See the file\n",
    "'popular-viewed-1.json' from the tabs above.)\n",
    "\n",
    "Your task is to modify the article_overview() function to process the saved\n",
    "file that represents the most popular articles (by view count) from the last\n",
    "day, and return a tuple of variables containing the following data:\n",
    "- labels: list of dictionaries, where the keys are the \"section\" values and\n",
    "  values are the \"title\" values for each of the retrieved articles.\n",
    "- urls: list of URLs for all 'media' entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview() function. See the test()\n",
    "function for examples of the elements of the output lists.\n",
    "The rest of functions are provided for your convenience, if you want to access\n",
    "the API by yourself.\n",
    "\"\"\"\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "\n",
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"68580a55ee9d40ecaceb790ab2a6d573\",\n",
    "            \"article\": \"68580a55ee9d40ecaceb790ab2a6d573\"}\n",
    "\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    print (filename)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "data = get_from_file(\"viewed\", 1)\n",
    "\n",
    "\n",
    "title = list()\n",
    "urls = list()\n",
    "labels = list()\n",
    "#print(data)\n",
    "#labels = {}\n",
    "#for article in data:\n",
    "    #print ((article[\"section\"]))\n",
    "    #print ((article[\"title\"]))\n",
    "    #labels[(article[\"section\"])] : article[\"title\"])\n",
    "labels = dict((article[\"section\"], article[\"title\"]) for article in data)\n",
    "print (labels)\n",
    "#for section, titles in labels.items():\n",
    "    #title.append(titles)\n",
    "#sections = list(article[\"section\"] for article in data)\n",
    "#titles = list(article[\"section\"] for article in data)\n",
    "urls = list(article[\"url\"] for article in data)\n",
    "print (len(urls))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    #print (data)\n",
    "    titles = list()\n",
    "    urls = list()\n",
    "    labels = dict(((article[\"section\"]), article[\"title\"]) for article in data)\n",
    "    #titles = list(title for section, title in labels.items())\n",
    "    print (labels.keys(), labels.values())\n",
    "    #urls = list(article[\"url\"] for article in data)\n",
    "\n",
    "    return (titles, urls)\n",
    "\n",
    "titles, urls = article_overview(\"viewed\", 1)\n",
    "\n",
    "#print ((data))\n",
    "'''\n",
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        #print \"You need to register for NYTimes Developer account to run this program.\"\n",
    "        #print \"See Intructor notes for information\"\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print (\"Time period can be 1,7, 30 days only\")\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print (\"kind can be only one of viewed/shared/emailed\")\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):\n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "\n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "\n",
    "        \n",
    "        '''\n",
    "def test():\n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    assert len(titles) == 20\n",
    "    assert len(urls) == 30\n",
    "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
    "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chilren of root:\n",
      "ui\n",
      "ji\n",
      "fm\n",
      "bdy\n",
      "bm\n",
      "('\\nTitle:\\n', 'Standardization of the functional syndesmosis widening by dynamic U.S examination')\n",
      "\n",
      "Author email addresses:\n",
      "omer@extremegate.com\n",
      "mcarmont@hotmail.com\n",
      "laver17@gmail.com\n",
      "nyska@internet-zahav.net\n",
      "kammarh@gmail.com\n",
      "gideon.mann.md@gmail.com\n",
      "barns.nz@gmail.com\n",
      "eukots@gmail.com\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "\n",
    "tree = ET.parse('exampleresearcharticle.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print (\"\\nChilren of root:\")\n",
    "for child in root: # child -> is the elements followed from the \"root\"\n",
    "    print (child.tag)\n",
    "\n",
    "    \n",
    "# Extract Titles\n",
    "title = root.find('./fm/bibl/title') # returns first element that matches\n",
    "title_text = \"\"\n",
    "for p in title:\n",
    "    title_text += p.text\n",
    "print (\"\\nTitle:\\n\", title_text)\n",
    "\n",
    "# Extract email from Authors\n",
    "print (\"\\nAuthor email addresses:\")\n",
    "for a in root.findall('./fm/bibl/aug/au'):# returns all elements that match\n",
    "    email = a.find('email')\n",
    "    if email is not None:\n",
    "        print (email.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None\n",
    "        }\n",
    "        \n",
    "        data[\"fnm\"] = author.find('./fnm').text\n",
    "        data[\"snm\"] = author.find('./snm').text\n",
    "        data[\"email\"] = author.find('./email').text\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "#root = get_root(article_file)\n",
    "get_authors(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys, but you have to extract the attributes from the \"insr\" tag\n",
    "# and add them to the list for the dictionary key \"insr\"\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        data[\"fnm\"] = author.find('./fnm').text\n",
    "        data[\"snm\"] = author.find('./snm').text\n",
    "        data[\"email\"] = author.find('./email').text\n",
    "    \n",
    "        \n",
    "        insr = author.findall('./insr')\n",
    "        for i in insr:\n",
    "            data[\"insr\"].append(i.attrib[\"iid\"])\n",
    "\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "#root = get_root(article_file)\n",
    "#get_authors(root)\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"insr\"] == solution[1][\"insr\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'AllMajors', 'ATL', 'BWI', 'BOS', 'CLT', 'MDW', 'ORD', 'DAL', 'DFW', 'DEN', 'DTW', 'FLL', 'IAH', 'LAS', 'LAX', 'MIA', 'MSP', 'JFK', 'LGA', 'EWR', 'MCO', 'PHL', 'PHX', 'PDX', 'SLC', 'SAN', 'SFO', 'SEA', 'TPA', 'DCA', 'IAD', 'AllOthers', 'UXM', 'ABR', 'ABI', 'DYS', 'ADK', 'VZF', 'BQN', 'AKK', 'KKI', 'AKI', 'AKO', 'CAK', '7AK', 'KQA', 'AUK', 'ALM', 'ALS', 'ABY', 'ALB', 'ABQ', 'ZXB', 'WKK', 'AED', 'AEX', 'AXN', 'AET', 'ABE', 'AIA', 'APN', 'DQH', 'AOO', 'AMA', 'ABL', 'OQZ', 'AOS', 'OTS', 'AKP', 'EDF', 'DQL', 'MRI', 'ANC', 'AND', 'AGN', 'ANI', 'ANN', 'ANB', 'ANV', 'ATW', 'ACV', 'ARC', 'ADM', 'AVL', 'HTS', 'ASE', 'AST', 'AHN', 'AKB', 'PDK', 'FTY', 'ACY', 'ATT', 'ATK', 'MER', 'AUO', 'AGS', 'AUG', 'AUS', 'A28', 'BFL', 'BGR', 'BHB', 'BRW', 'BTI', 'BQV', 'A2K', 'BTR', 'BTL', 'AK2', 'A56', 'BTY', 'BPT', 'BVD', 'WBQ', 'BKW', 'BED', 'A11', 'KBE', 'BLV', 'BLI', 'BLM', 'JVL', 'BVU', 'BJI', 'RDM', 'BEH', 'BET', 'BTT', 'BVY', 'OQB', 'A50', 'BIC', 'BIG', 'BGQ', 'BMX', 'PWR', 'A85', 'BIL', 'BIX', 'BGM', 'KBC', 'BHM', 'BIS', 'BYW', 'BID', 'BMG', 'BMI', 'BFB', 'BYH', 'BCT', 'BOI', 'RLU', 'BXS', 'BLD', 'BYA', 'BWG', 'BZN', 'BFD', 'A23', 'BRD', 'BKG', 'PWT', 'KTS', 'BDR', 'TRI', 'BKX', 'RBH', 'BRO', 'BWD', 'BQK', 'BCE', 'BKC', 'BUF', 'IFP', 'BUR', 'BRL', 'BTV', 'MVW', 'BNO', 'BTM', 'JQF', 'UXI', 'CDW', 'C01', 'ADW', 'CDL', 'CGI', 'LUR', 'EHM', 'CZF', 'A61', 'A40', 'CYT', 'MDH', 'CLD', 'CNM', 'A87', 'CPR', 'CDC', 'CID', 'JRV', 'NRR', 'CEM', 'CDR', 'CIK', 'CMI', 'WCR', 'CHS', 'CRW', 'SPB', 'STT', 'CHO', 'CYM', 'CHA', 'CYF', 'WA7', 'CEX', 'EGA', 'NCN', 'KCN', 'VAK', 'CYS', 'PWK', 'CHI', 'DPA', 'LOT', 'CKX', 'CIC', 'CEF', 'KCG', 'KCL', 'WQZ', 'KCQ', 'CZN', 'CIV', 'ZXH', 'SSB', 'STX', 'CHU', 'LUK', 'CVG', 'OQC', 'A12', 'CHP', 'IRC', 'CLP', 'CKB', 'BKL', 'CLE', 'CGF', 'CFT', 'CLK', 'ZXN', 'CVN', 'ZXI', 'OOB', 'COD', 'CFA', 'KCC', 'A69', 'CDB', 'CXF', 'CLL', 'KCR', 'COS', 'COA', 'COU', 'CAE', 'CSG', 'CBM', 'GTR', 'OSU', 'CMH', 'LCK', 'CCR', 'CKU', 'CDV', 'CBA', 'CRP', 'CEZ', 'CVO', 'CIL', 'CGA', 'CEC', 'CKD', 'CUW', 'CPX', 'CBE', 'DCK', 'ADS', 'RBD', 'AFW', 'FTW', 'DGB', 'DAN', 'WQW', 'MGY', 'DAY', 'DAB', 'AA8', 'SCC', 'FVZ', 'A02', 'DTH', 'DTR', 'DCU', 'DEC', 'XXV', 'A36', 'DHB', 'DRG', 'DRT', 'DLF', 'DJN', 'DMN', 'DTO', 'APA', 'FTG', 'DSM', 'DSI', 'DTL', 'DET', 'DTT', 'YIP', 'DVL', 'DIK', 'DLG', 'DIO', 'DDC', 'FVQ', 'DOF', 'DHN', 'DOV', 'DRF', 'A22', 'FQQ', 'DUJ', 'DBQ', 'DLH', 'A4K', 'AMK', 'DRO', 'EAA', 'EGE', 'FRG', 'HTO', 'GA0', 'ESN', 'ESD', 'EAU', 'EDA', 'EDW', 'EEK', 'EGX', 'KKU', 'KEK', 'ZXO', 'IPL', 'ELD', 'BIF', 'ELP', 'ELV', 'ELI', 'EKO', 'ELM', 'LYU', 'ELY', 'EMK', 'WDG', 'ERI', 'ESC', 'EUG', 'EVV', 'EVM', 'PAE', 'EXI', 'EIL', 'FAI', 'FBK', 'A01', 'A6K', 'SUU', 'FAJ', 'KFP', 'FWL', 'FAR', 'FMN', 'FYV', 'XNA', 'FAY', 'POB', 'FFM', 'FIC', 'FAQ', 'FLG', 'FNT', 'FLO', 'FNL', 'WRI', 'FOD', 'FQW', 'FHU', 'TBN', 'RSW', 'FPR', 'FSI', 'FSM', 'FWA', 'FWH', 'FYU', 'FKL', 'VZE', 'FAT', 'FRD', 'FBS', 'FNR', 'GNV', 'GVL', 'GBH', 'GAL', 'GUP', 'GAM', 'GEK', 'GCK', 'GYY', 'GCC', 'AQY', 'GGW', 'AZ3', 'GDV', 'AK6', 'FVW', 'GLV', 'GNU', 'GYR', 'FVX', 'JGC', 'GCN', 'AZ1', 'GFK', 'GRI', 'GJT', 'GRR', 'GPZ', 'VWZ', 'GMT', 'XWA', 'KGX', 'GBD', 'GTF', 'GRB', 'GSO', 'GLH', 'PGV', 'GVT', 'GSP', 'UAM', 'GUM', 'GUF', 'GPT', 'GKN', 'GUC', 'GST', 'HGR', 'HNS', 'A03', 'HNM', 'CMX', 'VWD', 'ZXJ', 'HRL', 'MDT', 'HRO', 'BDL', 'PIB', 'HVR', 'HWI', 'HHR', 'HDN', 'HYS', 'HKB', 'HLN', 'T2X', 'HES', 'HIB', 'HKY', 'HIO', 'ITO', 'HHH', 'HBH', 'HOB', 'HGZ', 'HOL', 'HYL', 'HCR', 'HOM', 'HST', 'VWX', 'HNL', 'MKK', 'HNH', 'HPB', 'HOP', 'HOT', 'DWH', 'EFD', 'HOU', 'HUS', 'HSV', 'HON', 'HSL', 'HUT', 'HYA', 'HYG', 'WHD', 'ICY', 'IDA', 'IGG', 'ILI', 'ZXF', 'IND', 'MQJ', 'INL', 'A57', 'IYK', 'IMT', 'IWD', 'ISP', 'SAW', 'ITH', 'KIB', 'A59', 'A26', 'MKL', 'JAC', 'JAN', 'NZC', 'JAX', 'NIP', 'OAJ', 'JMS', 'JHW', 'VZM', 'JON', 'JST', 'JBR', 'JLN', 'JNU', 'OGG', 'KAE', 'A37', 'A35', 'KKK', 'AZO', 'LUP', 'FCA', 'KLG', 'KAL', 'MUE', 'KNB', 'MKC', 'MCI', 'JHM', 'JRF', 'KKL', 'A65', 'KYK', 'KXA', 'KUK', 'VZR', 'VZY', 'FQD', 'VIK', 'MVM', 'EAR', 'EEN', 'ENA', 'KEH', 'KTN', 'WFB', 'DQU', 'EYW', 'NQX', 'QQB', 'IAN', 'GRK', 'ILE', 'A29', 'KVC', 'AKN', 'IGM', 'ISO', 'KPN', 'IRK', 'KKB', 'KVL', 'KZH', '06A', 'LMT', 'KLW', 'SZL', 'TYS', 'OBU', 'A43', 'ADQ', 'KDK', 'A41', 'KNK', 'KGK', 'KOA', 'KKH', 'KOT', 'OTZ', 'KKA', 'KYU', 'LKK', 'UUK', 'KWT', 'KWK', 'LSE', 'LAF', 'LFT', 'LCH', 'XXW', 'HII', 'LMA', 'TVL', 'LNY', 'ZXK', 'WJF', 'LNS', 'LAN', 'LAR', 'LRD', 'KLN', 'HSH', 'LSV', 'VGT', 'LBE', 'LZU', 'LAW', 'ALZ', 'LEB', 'VA4', 'KLL', 'LWB', 'LWS', 'LEW', 'LWT', 'LEX', 'LBL', 'LIH', 'UXA', 'LVD', 'LNK', 'LIT', '05A', 'LGU', 'LNI', 'LGB', 'LIJ', 'GGG', 'LPS', 'LPR', 'LAM', 'SDF', 'LBB', 'LYH', 'MCN', 'MSN', 'A75', 'MMH', 'MNZ', 'MHT', 'MHK', 'MBL', 'MLY', 'KMO', 'MZJ', 'MTH', 'MYH', 'MWA', 'MQT', 'MLL', 'MVY', 'MCW', 'MSS', 'MYK', 'MAZ', 'MYL', 'MXY', 'MCK', 'OQA', 'MCG', 'MCL', 'MFR', 'MDR', 'MYU', 'MLB', 'OQL', 'MEM', 'XXX', 'MCE', 'MEI', 'OQM', 'MFH', 'MTM', 'WMK', 'MPB', '6B0', 'MDO', 'MAF', 'MDY', 'MLS', 'NQA', 'MKE', 'MHM', 'MWL', 'STP', 'MIB', 'MOT', 'MNT', 'MFE', 'MSO', 'CNY', 'BFM', 'MOB', 'MOD', 'VZG', 'MLI', 'MLU', 'MRY', 'MGM', 'MTJ', 'UXR', 'MGW', 'MMU', 'MVL', 'KMY', 'MWH', 'MOS', 'CWA', 'MUO', 'NUQ', 'MOU', 'A13', 'MSL', 'VZC', 'MKG', 'MYR', 'NNK', 'WQR', 'AA2', 'ACK', 'KEB', 'APC', 'WNA', 'KPM', 'PKA', 'APF', 'BNA', 'NKI', 'NLG', 'ENN', 'EWB', 'EWN', 'HVN', 'ARA', 'GON', 'NEW', 'MSY', 'DQN', 'KNW', 'JRB', 'TSS', 'NYC', 'SWF', 'LFI', 'PHF', 'ONP', 'WWT', 'EWK', 'IAG', 'NME', 'NIB', 'IKO', 'NIN', 'RQI', 'WTK', 'OME', 'NNL', 'ORV', 'OFK', 'ORF', 'NGU', 'OTH', 'LBF', 'MA5', 'OHC', 'ORT', 'NUI', 'NUL', 'NUP', 'ZNC', 'ODW', 'OAK', 'OCF', 'OFU', 'HIF', 'OGD', 'OGS', 'OKC', 'OJC', 'JCI', 'OLH', 'KOY', 'XWS', 'OLV', 'OLM', 'OMA', 'ONN', 'ONT', 'OPH', 'ORL', 'OSH', 'KOZ', 'OWB', 'UOX', 'OXR', 'PBK', 'PAH', 'PGA', 'PPG', 'PCE', 'PSP', 'PMD', 'PAQ', 'PFN', 'ECP', 'PAM', 'PKD', 'PKB', 'PSC', 'PRB', 'DQR', '1G4', 'DQW', 'WQJ', 'PDB', 'PEC', 'PLN', 'PDT', 'PNS', 'NPA', 'PIA', 'KPV', 'VYS', 'GUS', 'PSG', 'PNF', 'PNE', 'LUF', 'DVT', 'AZA', 'SCF', 'PIR', 'PIP', 'UGB', 'PQS', 'SOP', 'AGC', 'PIT', 'PSF', 'PTU', 'PLB', 'PBG', 'PTR', 'PIH', 'A27', 'KPB', 'PHO', 'PIZ', 'POQ', 'PNC', 'PSE', 'PTK', 'PVY', 'PTD', 'PTC', 'PTA', 'CLM', 'KPY', 'KPC', 'PGM', 'PTH', 'A48', 'ORI', 'PML', 'PPV', 'TWD', 'A17', 'KPR', 'PCA', 'WQU', 'PTV', 'PWM', 'PSM', 'PRC', 'PQI', 'PUC', 'BLF', 'PPC', 'PVD', 'PVC', 'PVU', 'PUO', 'A39', 'PUB', 'PUW', 'OQP', 'PGD', 'AK5', 'UIN', 'KWN', 'RDU', 'RMP', 'RCA', 'RAP', 'RDG', 'RDV', 'RDB', 'A76', 'A04', 'RDR', 'RDD', 'RNO', 'RNT', 'RHI', 'RIC', 'RIL', 'RIV', 'RIW', 'ROA', 'RCE', 'RST', 'ROC', 'RKS', 'RFD', 'RKD', 'RWI', 'ROG', 'FAL', 'RME', 'RSJ', 'ROW', 'ROP', 'RBY', 'RUI', 'RSH', 'RSN', 'RUT', 'SAC', 'SMF', 'SAD', 'MBS', 'SPN', 'SLE', 'SLT', 'SLN', 'SNS', 'SBY', 'SMN', 'ZXM', 'SJT', 'SKF', 'SAT', 'NKX', 'MYF', 'NZY', 'SJC', 'WSJ', 'SIG', 'SJU', 'SBP', 'SDP', 'KSR', 'OQS', 'SFB', 'SNA', 'SBA', 'SAF', 'SMX', 'STS', 'SLK', 'SRQ', 'CIU', 'SVN', 'SAV', 'SVA', 'SCM', 'BFF', 'AVP', 'SYB', 'BFI', 'LKE', 'SDX', 'A07', 'WLK', 'SOV', 'A31', 'SQV', 'SWD', 'SHX', 'SKK', 'A90', 'A77', 'SXP', 'SYA', 'SHR', 'PNX', 'OQV', 'SHH', 'SOW', 'BAD', 'SHV', 'SHG', 'SDY', 'SVC', 'SUX', 'FSD', 'NKT', 'SIT', 'SKJ', 'SGY', 'SKW', 'SLQ', 'SCJ', 'MQY', 'SXQ', 'SBN', 'WSN', 'SVW', 'GEG', 'SPI', 'SGF', 'UST', 'STC', 'STG', 'SGU', 'STJ', 'CPS', 'STL', 'SUS', 'KSM', 'SMK', 'SNP', 'PIE', 'RMN', 'STF', 'SCE', 'SHD', 'WSB', 'WBB', 'WA6', 'VZO', 'SVS', 'SWO', 'SCK', 'SRV', 'SSC', 'SUN', 'SYR', 'TCM', 'TIW', 'TCT', 'TKA', 'TLH', 'MCF', 'TAL', 'TSM', 'TLJ', 'TEK', 'TAV', 'TWE', 'TLF', 'TLA', 'TEX', 'TKE', 'HUF', 'A30', 'TEB', 'TEH', 'TXK', 'DLS', 'TVF', 'KTB', 'TNC', 'TIQ', 'TOG', 'TKJ', 'TKI', 'OOK', 'TOL', 'TPH', 'FOE', 'JZE', 'TVC', 'TTN', 'TTD', 'TUS', 'TUL', 'TLT', 'UTM', 'WTL', 'TNK', 'TUP', 'TCL', 'TWF', 'TWA', 'TYR', 'TYE', 'UGI', 'UGS', 'UMT', 'UMB', 'UNK', 'DUT', 'UTO', 'VDZ', 'VLD', 'VPS', 'VPZ', 'VNY', 'VUO', 'VEE', 'VEL', 'VRB', 'VCT', 'VCV', 'A67', 'VQS', 'VCB', 'A70', 'VIS', 'OQI', 'CNW', 'ACT', 'AIN', 'AWK', 'WAA', 'ALW', 'WWA', 'OXC', 'KWF', 'ALO', 'ART', 'ATY', 'EAT', 'ENV', 'VT1', 'AWM', 'PBI', 'KWP', 'WYS', 'WST', 'BAF', 'FOK', 'WSX', 'WWP', 'WMO', 'HPN', 'DQS', 'SPS', 'ICT', 'WDB', 'VZN', 'IPT', 'ISN', 'WOW', 'ILG', 'ILM', 'ILN', 'WGO', 'INW', 'INT', 'WA5', 'WSM', 'OLF', 'ORH', 'WRL', 'WRG', 'YKM', 'YAK', 'XWC', 'WYB', 'YNG', 'A63', 'NYL', 'YUM', 'KZB', 'AK8']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def options(soup, id):\n",
    "    option_values = []\n",
    "    carrier_list = soup.find(id=id)\n",
    "    for option in carrier_list.find_all('option'):\n",
    "        option_values.append(option['value'])\n",
    "    return option_values   \n",
    "\n",
    "soup = BeautifulSoup(open(\"virgin_and_logan_airport.html\"))\n",
    "codes = options(soup, 'AirportList') #Change to 'CarrierList' from 'Carrier_List' \n",
    "print(codes)\n",
    "\n",
    "def print_list(label, codes):\n",
    "    print (\"\\n%s:\" % label)\n",
    "    for c in codes:\n",
    "        print c\n",
    "#print_list(\"Airports\", codes)\n",
    "\n",
    "def print_list(label, codes):\n",
    "    print (\"\\n%s:\" % label)\n",
    "    for c in codes:\n",
    "        print c\n",
    "        \n",
    "        \n",
    "def main():\n",
    "    soup = BeautifulSoup(open(\"virgin_and_logan_airport.html\"))\n",
    "    \n",
    "    codes = options(soup, 'CarrierList')\n",
    "    print_list(\"Carriers\", codes)\n",
    "    \n",
    "    #codes = options(soup, 'AirportList')\n",
    "    #print_list(\"Airports\", codes)\n",
    "    \n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"page_source.html\"\n",
    "\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        #pass\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        ev = soup.find(id=\"__EVENTVALIDATION\")\n",
    "        print (ev)\n",
    "        data[\"eventvalidation\"] = ev[\"value\"]\n",
    "        \n",
    "        vs = soup.find(id=\"__VIEWSTATE\")\n",
    "        data[\"viewstate\"] = vs[\"value\"]\n",
    "\n",
    "    return data\n",
    "\n",
    "#extract_data(html_page)\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "r = requests.get(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")\n",
    "soup = BeautifulSoup(r.text)\n",
    "viewstate_element = soup.find(id=\"__VIEWSTATE\")\n",
    "viewstate = viewstate_element[\"value\"]\n",
    "eventvalidation_element = soup.find(id=\"__EVENTVALIDATION\")\n",
    "eventvalidation = eventvalidation_element[\"value\"]\n",
    "viewstategenerator_element = soup.find(id=\"__VIEWSTATEGENERATOR\")\n",
    "viewstategenerator = viewstategenerator_element[\"value\"]\n",
    "\n",
    "r = requests.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate,\n",
    "                          \"__VIEWSTATEGENERATOR\": viewstategenerator\n",
    "                    })\n",
    "\n",
    "# Doesnt' produce ERROR\n",
    "f = open(\"virgin_and_logan_airport2.html\", \"w\")\n",
    "f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution from Instruction - Scraping Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "## Use \"requests.Session()\" to solve the ERROR\n",
    "s = requests.Session() # Because of Cookie\n",
    "\n",
    "## Use \"..requests.get..\" to replicate error\n",
    "#r = requests.get(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")\n",
    "r = s.get(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")\n",
    "soup = BeautifulSoup(r.text)\n",
    "viewstate_element = soup.find(id=\"__VIEWSTATE\")\n",
    "viewstate = viewstate_element[\"value\"]\n",
    "eventvalidation_element = soup.find(id=\"__EVENTVALIDATION\")\n",
    "eventvalidation = eventvalidation_element[\"value\"]\n",
    "viewstategenerator_element = soup.find(id=\"__VIEWSTATEGENERATOR\")\n",
    "viewstategenerator = viewstategenerator_element[\"value\"]\n",
    "\n",
    "## Use \"..requests.get..\" to replicate error\n",
    "#r = requests.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")\n",
    "r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "           data = (\n",
    "                   (\"__EVENTTARGET\", \"\"),\n",
    "                   (\"__EVENTARGUMENT\", \"\"),\n",
    "                   (\"__VIEWSTATE\", viewstate),\n",
    "                   (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                   (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                   (\"CarrierList\", \"VX\"),\n",
    "                   (\"AirportList\", \"BOS\"),\n",
    "                   (\"Submit\", \"Submit\")\n",
    "                  ))\n",
    "\n",
    "f = open(\"virgin_and_logan_airport_REPLICATE_ERROR.html\", \"w\")\n",
    "f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Quiz 1: Carrier List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "#option_values = []\n",
    "#    carrier_list = soup.find(id=id)\n",
    "#    for option in carrier_list.find_all('option'):\n",
    "#        option_values.append(option['value'])\n",
    "#    return option_values   \n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id='CarrierList')\n",
    "        for carrier in carrier_list.find_all('option'):\n",
    "            if not carrier['value'].startswith('All'):\n",
    "                data.append(carrier['value'])\n",
    "        #print(data)\n",
    "    return data\n",
    "\n",
    "#extract_carriers(html_page)\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert len(data) == 16\n",
    "    assert \"FL\" in data\n",
    "    assert \"NK\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Quiz 2: Airport List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        airport_list = soup.find(id='AirportList')\n",
    "        for airport in airport_list.find_all('option'):\n",
    "            if not airport['value'].startswith('All'):\n",
    "                data.append(airport['value'])\n",
    "        #print(data)\n",
    "    return data\n",
    "\n",
    "#extract_airports(html_page)\n",
    "def test():\n",
    "    data = extract_airports(html_page)\n",
    "    assert len(data) == 15\n",
    "    assert \"ATL\" in data\n",
    "    assert \"ABR\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Quiz 3: Processing All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get.\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    print \"Running a simple test...\"\n",
    "    open_zip(datadir)\n",
    "    files = process_all(datadir)\n",
    "    data = []\n",
    "    # Test will loop over three data files.\n",
    "    for f in files:\n",
    "        data += process_file(f)\n",
    "        \n",
    "    assert len(data) == 399  # Total number of rows\n",
    "    for entry in data[:3]:\n",
    "        assert type(entry[\"year\"]) == int\n",
    "        assert type(entry[\"month\"]) == int\n",
    "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
    "        assert len(entry[\"airport\"]) == 3\n",
    "        assert len(entry[\"courier\"]) == 2\n",
    "    assert data[0][\"courier\"] == 'FL'\n",
    "    assert data[0][\"month\"] == 10\n",
    "    assert data[-1][\"airport\"] == \"ATL\"\n",
    "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
    "    \n",
    "    print \"... success!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
